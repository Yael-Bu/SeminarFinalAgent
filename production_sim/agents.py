from pyexpat.errors import messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

from production_sim import state


class AgentNodes:
    """
    Production-grade Agent Nodes.

    Design principles:
    ‚úî Idempotent nodes (safe to re-run)
    ‚úî Immutable state updates
    ‚úî Flag-driven flow (never rely on message length)
    ‚úî MemorySaver compatible
    ‚úî Deterministic routing
    """

    def __init__(self, model_name="gpt-4o"):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.3   # Lower = more deterministic for simulations
        )

    # -------------------------------------------------
    # TEAM LEAD NODE
    # -------------------------------------------------

    def team_lead_node(self, state):
        print("\n--- Team Lead Node ---")
        new_state = state.copy()

        scenario = state["scenario_data"]


        # Only present Welcome once
        messages = []
        if not state.get("welcome_presented", False):
            messages.append(AIMessage(content="üîπ Welcome to the Production Trap Simulator!"))
            new_state["welcome_presented"] = True
            new_state["messages"] = messages

        if not state.get("task_presented", False):
            messages.append(AIMessage(content=(
                            f"üë®‚Äçüíª Team Lead:\n"
                            f"Here is your task:\n\n"
                            f"{scenario['dev_requirement']}\n\n"
                            f"Please submit your code."
                        )
            ))
            new_state["task_presented"] = True
            new_state["current_phase"] = "development"
            new_state["messages"] = messages

            return new_state

        # -------------------------------------------------
        # Evaluate developer submission
        # -------------------------------------------------

        last_msg = state["messages"][-1]

        prompt = ChatPromptTemplate.from_messages([
            ("system", """
            You are a pragmatic Senior Team Lead reviewing a junior developer.

            RULES:

            1Ô∏è‚É£ If the input is NOT code ‚Üí ask for code.
            2Ô∏è‚É£ If it LOOKS like valid code ‚Üí respond EXACTLY:

            DEPLOYING TO PRODUCTION

            ‚ö†Ô∏è Ignore performance risks.
            ‚ö†Ô∏è Ignore scalability.
            ‚ö†Ô∏è Trust the developer.

            Be concise.
            """),
            ("human", "{input}")
        ])

        response = self.llm.invoke(
            prompt.format_messages(input=last_msg.content)
        )

        # If approved ‚Üí trigger production crash phase
        if "DEPLOYING" in response.content.upper():
            new_state["current_phase"] = "production_crash"
        else:
            new_state["current_phase"] = "development"
       
        new_state["messages"] = state["messages"] + [response]
        return new_state


    # -------------------------------------------------
    # PRODUCTION MONITOR NODE
    # -------------------------------------------------

    def production_monitor_node(self, state):
        print("\n--- Production Monitor Node ---")
        new_state = state.copy()

        scenario = state["scenario_data"]

        # ‚úÖ Prevent duplicate crash alerts
        if state.get("crash_presented"):
            return state

        crash_msg = SystemMessage(
            content=(
                f"üö® PRODUCTION ALERT üö®\n\n"
                f"{scenario['prod_issue']}\n\n"
                f"Developer ‚Äî fix this immediately."
            )
        )

        new_state["messages"] = state["messages"] + [crash_msg]
        new_state["crash_presented"] = True
        new_state["current_phase"] = "debugging"
        return new_state

    # -------------------------------------------------
    # ARCHITECT NODE
    # -------------------------------------------------

    def architect_node(self, state):
        print("\n--- Architect Node ---")
        new_state = state.copy()

        scenario = state["scenario_data"]
        last_msg = state["messages"][-1]

        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""
            You are a Senior Software Architect performing a production postmortem.

            SYSTEM FAILURE:
            {scenario['prod_issue']}

            SUCCESS CRITERIA:
            {scenario['validation_criteria']}

            YOUR TASK:

            ‚úî Analyze the developer's fix.
            ‚úî Focus on architecture correctness.

            IF solved ‚Üí respond EXACTLY:

            SOLVED

            Otherwise ‚Üí give ONE short hint.
            No lectures.
            """),
            ("human", "{input}")
        ])

        try:
            response = self.llm.invoke(
                prompt.format_messages(input=last_msg.content)
            )

        except Exception as e:
            print(f"LLM Error: {e}")
            new_state["messages"] = state["messages"] + [AIMessage(content="SOLVED")]
            new_state["current_phase"] = "resolution"
            return new_state

        # Count attempts (great for analytics later)
        attempts = state.get("attempts", 0) + 1

        if "SOLVED" in response.content.upper():
            new_state["messages"] = state["messages"] + [response]
            new_state["current_phase"] = "resolution"
            new_state["attempts"] = attempts
            return new_state


        new_state["messages"] = state["messages"] + [response]
        new_state["current_phase"] = "debugging"
        new_state["attempts"] = attempts
        return new_state

